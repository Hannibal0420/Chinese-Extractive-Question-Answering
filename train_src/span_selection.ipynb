{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "train_df = pd.read_json('../ADL_HW1/datasets/train.json')\n",
    "valid_df = pd.read_json('../ADL_HW1/datasets/valid.json')\n",
    "context_df = pd.read_json('../ADL_HW1/datasets/context.json')\n",
    "\n",
    "def reformat(df):\n",
    "    # Create new columns based on the existing ones\n",
    "    df['answers'] = df['answer']\n",
    "\n",
    "    # Find the index of 'relevant' in 'paragraphs'\n",
    "    df['context'] = df['relevant'].apply(lambda x: context_df.iloc[x])\n",
    "\n",
    "    # Apply the transformation\n",
    "    df['answers'] = df['answers'].apply(lambda x: {'text': [x['text']],'answer_start': [x['start']]})\n",
    "\n",
    "    # Drop the original columns if needed\n",
    "    df.drop(['answer', 'paragraphs', 'relevant'], axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "train_df = reformat(train_df)\n",
    "valid_df = reformat(valid_df)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "valid_dataset = datasets.Dataset.from_pandas(valid_df)\n",
    "raw_dataset = datasets.DatasetDict({\"train\":train_dataset,\"validation\":valid_dataset})\n",
    "\n",
    "column_names = raw_dataset[\"train\"].column_names\n",
    "print('\\n', raw_dataset)\n",
    "print('\\n', column_names)\n",
    "\n",
    "raw_dataset.save_to_disk('./tmp/raw_dataset_SS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./run_qa_no_trainer.py \\\n",
    "  --model_name_or_path hfl/chinese-roberta-wwm-ext \\\n",
    "  --dataset_name ./tmp/raw_dataset_SS \\\n",
    "  --max_seq_length 512 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --gradient_accumulation_steps 4 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --output_dir ./PS_model/ \\"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
