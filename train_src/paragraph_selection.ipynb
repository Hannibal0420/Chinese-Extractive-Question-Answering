{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datasets\n",
    "\n",
    "train_df = pd.read_json('../ADL_HW1/datasets/train.json')\n",
    "valid_df = pd.read_json('../ADL_HW1/datasets/valid.json')\n",
    "context_df = pd.read_json('../ADL_HW1/datasets/context.json')\n",
    "# print(train_df.iloc[2])\n",
    "\n",
    "def reformat(df, have_label=0):\n",
    "    # Create new columns based on the existing ones\n",
    "    df['sent1'] = df['question']\n",
    "    df['sent2'] = ''\n",
    "\n",
    "    # Extract elements from the 'paragraphs' list into new columns\n",
    "    df['ending0'] = df['paragraphs'].apply(lambda x: context_df.iloc[x[0], 0])\n",
    "    df['ending1'] = df['paragraphs'].apply(lambda x: context_df.iloc[x[1], 0])\n",
    "    df['ending2'] = df['paragraphs'].apply(lambda x: context_df.iloc[x[2], 0])\n",
    "    df['ending3'] = df['paragraphs'].apply(lambda x: context_df.iloc[x[3], 0])\n",
    "\n",
    "    # Find the index of 'relevant' in 'paragraphs'\n",
    "    if have_label:\n",
    "        df['label'] = df.apply(lambda row: row['paragraphs'].index(row['relevant']), axis=1)\n",
    "        df.drop(['relevant'], axis=1, inplace=True)\n",
    "\n",
    "    df.drop(['question', 'paragraphs'], axis=1, inplace=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "train_df = reformat(train_df, have_label=1)\n",
    "valid_df = reformat(valid_df, have_label=1)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(train_df)\n",
    "valid_dataset = datasets.Dataset.from_pandas(valid_df)\n",
    "raw_dataset = datasets.DatasetDict({\"train\":train_dataset,\"validation\":valid_dataset})\n",
    "\n",
    "column_names = raw_dataset[\"train\"].column_names\n",
    "print('\\n', raw_dataset)\n",
    "print('\\n', column_names)\n",
    "\n",
    "raw_dataset.save_to_disk('./tmp/raw_dataset_PS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tune existing model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python ./run_swag_no_trainer.py \\\n",
    "  --model_name_or_path bert-base-chinese \\\n",
    "  --dataset_name ./tmp/raw_dataset_PS \\\n",
    "  --max_seq_length 512 \\\n",
    "  --per_device_train_batch_size 1 \\\n",
    "  --learning_rate 3e-5 \\\n",
    "  --num_train_epochs 2 \\\n",
    "  --gradient_accumulation_steps 2 \\\n",
    "  --output_dir ./PS_model/ \\"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
